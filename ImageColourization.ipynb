{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBNIY0sWqXMg"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import cv2\n",
        "from keras.layers import Input,Dense,Reshape,Conv2D,Dropout,multiply,Dot,Concatenate,subtract,ZeroPadding2D\n",
        "from keras.layers import BatchNormalization,LeakyReLU,Flatten\n",
        "from keras.layers import Conv2DTranspose as Deconv2d\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from google.colab import files\n",
        "from keras import backend as K\n",
        "import smtplib\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from google.colab import drive\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRASEsOm1hPz"
      },
      "outputs": [],
      "source": [
        "def plot(A,B,C,n):\n",
        "\n",
        "    samples = [A,B,C]\n",
        "    fig = plt.figure(figsize=(3,n))\n",
        "    gs = gridspec.GridSpec(3,n)\n",
        "    g=0\n",
        "    for i in range(3):\n",
        "        for j in range(n):\n",
        "            ax = plt.subplot(gs[g])\n",
        "            g+=1\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            if samples[i][j].shape == (32,32,1):\n",
        "              plt.imshow(samples[i][j].reshape(32, 32))\n",
        "            else:\n",
        "              plt.imshow(samples[i][j].reshape(32,32,3))\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1702nk3S3AI"
      },
      "outputs": [],
      "source": [
        "#for plotting any two images in case\n",
        "\n",
        "def ploty(A,B,n):\n",
        "\n",
        "    samples = [A,B]\n",
        "    fig = plt.figure(figsize=(3,n))\n",
        "    gs = gridspec.GridSpec(3,n)\n",
        "    g=0\n",
        "    for i in range(2):\n",
        "        for j in range(n):\n",
        "            ax = plt.subplot(gs[g])\n",
        "            g+=1\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            if samples[i][j].shape == (32,32,1):\n",
        "              plt.imshow(samples[i][j].reshape(32, 32, 1))\n",
        "            else:\n",
        "              plt.imshow(samples[i][j].reshape(32,32,3))\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "N68yNhL9z3jr",
        "outputId": "1383e800-164d-4814-ca0d-8da144dea4d4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fbb62156802f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btWe5_6_y5tr"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "y=x_train\n",
        "x=np.sum(y, axis=3)/(3*255)\n",
        "\n",
        "y_test=x_test\n",
        "x_test=np.sum(x_test, axis=3)/(3*255)   #for converting RGB into singe channel\n",
        "x_test=x_test.reshape(10000, 32, 32, 1)\n",
        "\n",
        "\n",
        "y=x_train/255\n",
        "y=y*2-1\n",
        "\n",
        "\n",
        "#x=x*2-1\n",
        "#x=np.dot(y[...,:3], [0.299, 0.587, 0.114])/255\n",
        "#x=x.reshape(50000,32, 32,1)\n",
        "\n",
        "x=x.reshape(50000, 32, 32, 1)\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maALbVf-qmm0"
      },
      "outputs": [],
      "source": [
        "\n",
        "x_shape=(32,32,1)\n",
        "y_shape=(32,32,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwbiitZCqo7r"
      },
      "outputs": [],
      "source": [
        "def Generator():\n",
        "  X = Input(shape = x_shape)\n",
        "\n",
        "  #C1 = ZeroPadding2D(padding=(1,1))(X)\n",
        "  C1 = Conv2D(64,kernel_size = 1, strides = 1,input_shape = x_shape)(X)\n",
        "  C1 = LeakyReLU(0.2)(C1)\n",
        "\n",
        "  C2 = Conv2D(128,kernel_size = 2, strides = 2)(C1)\n",
        "  C2 = LeakyReLU(0.2)(C2)\n",
        "\n",
        "  C3 = Conv2D(256,kernel_size = 2, strides = 2)(C2)\n",
        "  C3 = LeakyReLU(0.2)(C3)\n",
        "\n",
        "  C4 = Conv2D(512,kernel_size = 2, strides = 2)(C3)\n",
        "  C4 = LeakyReLU(0.2)(C4)\n",
        "\n",
        "  C5 = Conv2D(512, kernel_size = 2, strides = 2)(C4)\n",
        "  C5 = LeakyReLU(0.2)(C5)\n",
        "\n",
        "\n",
        "  DC0 = Deconv2d(512, kernel_size = 2, strides = 2)(C5)\n",
        "  DC0 = LeakyReLU(0.2)(DC0)\n",
        "  DC0 = BatchNormalization()(DC0)\n",
        "  DC0 = Dropout(0.5)(DC0)\n",
        "  DC0 = Concatenate(axis=3)([DC0, C4])\n",
        "\n",
        "\n",
        "  DC1 = Deconv2d(256,kernel_size=2, strides = 2)(DC0)\n",
        "  DC1 = LeakyReLU(0.2)(DC1)\n",
        "  DC1 = BatchNormalization()(DC1)\n",
        "  DC1 = Dropout(0.5)(DC1)\n",
        "  DC1 = Concatenate(axis=3)([DC1,C3])\n",
        "\n",
        "\n",
        "  DC2 = Deconv2d(128,kernel_size=2, strides = 2)(DC1)\n",
        "  DC2 = LeakyReLU(0.2)(DC2)\n",
        "  DC2 = BatchNormalization()(DC2)\n",
        "  DC2 = Concatenate(axis=3)([DC2,C2])\n",
        "\n",
        "  DC3 = Deconv2d(64,kernel_size=2, strides = 2)(DC2)\n",
        "  DC3 = LeakyReLU(0.2)(DC3)\n",
        "  DC3 = BatchNormalization()(DC3)\n",
        "  DC3 = Concatenate(axis=3)([DC3,C1])\n",
        "\n",
        "  #DC4 = ZeroPadding2D(padding=(3,1))(DC3)\n",
        "  CC4 = Conv2D(3,kernel_size=(1, 1), strides = (1, 1), activation=\"tanh\")(DC3)\n",
        "\n",
        "  m = Model(X,CC4)\n",
        "  #m.summary()\n",
        "  return m\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4X4dnGFqtKb"
      },
      "outputs": [],
      "source": [
        "def Discriminator():\n",
        "  X = Input(shape = x_shape)\n",
        "  Y = Input(shape = y_shape)\n",
        "\n",
        "  In = Concatenate(axis=3)([X,Y])\n",
        "\n",
        "  C1 = Conv2D(64,kernel_size = 2, strides = 2,input_shape = x_shape)(In)\n",
        "  C1 = BatchNormalization()(C1)\n",
        "  C1 = LeakyReLU(0.2)(C1)\n",
        "  C2 = Conv2D(128,kernel_size = 2, strides = 2)(C1)\n",
        "  C2 = BatchNormalization()(C2)\n",
        "  C2 = LeakyReLU(0.2)(C2)\n",
        "\n",
        "  C3 = Conv2D(256,kernel_size = 2, strides = 2)(C2)\n",
        "  C3 = BatchNormalization()(C3)\n",
        "  C3 = LeakyReLU(0.2)(C3)\n",
        "\n",
        "  C4 = Conv2D(512,kernel_size = 1, strides = 1)(C3)\n",
        "  C4 = BatchNormalization()(C4)\n",
        "  C4 = LeakyReLU(0.2)(C4)\n",
        "\n",
        "  D = Flatten()(C4)\n",
        "  D = Dense(128)(D)\n",
        "  D = Dense(1,activation='sigmoid')(D)\n",
        "\n",
        "  m = Model([X,Y],D)\n",
        "  #m.summary()\n",
        "  return m\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSmYt36HqvHi"
      },
      "outputs": [],
      "source": [
        "X = Input(shape = x_shape)\n",
        "Y = Input(shape = y_shape)\n",
        "\n",
        "gen = Generator()\n",
        "dis = Discriminator()\n",
        "\n",
        "out = gen(X)\n",
        "comb = dis([X,out])\n",
        "\n",
        "out = Flatten()(out)\n",
        "org = Flatten()(Y)\n",
        "\n",
        "cos_dis = Dot(axes = 1,normalize = True)([out,org])\n",
        "\n",
        "combined = Model([X,Y],[comb,cos_dis])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aEF_YjzLI9l"
      },
      "outputs": [],
      "source": [
        "genLoss=[]\n",
        "disLoss=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C005zvKC1VGD"
      },
      "outputs": [],
      "source": [
        "epochs = 300\n",
        "batch_size = 50\n",
        "n_example = 50000\n",
        "batches = int(n_example/batch_size)\n",
        "dis_updates = 2\n",
        "gen_updates = 1\n",
        "zero=np.zeros((batch_size,1))\n",
        "one=np.ones((batch_size,1))*0.9\n",
        "d_loss_factor = batches*2*dis_updates\n",
        "g_loss_factor = batches*gen_updates\n",
        "reuse = False\n",
        "adams = Adam(lr = 0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itR-uzoRVq_C"
      },
      "outputs": [],
      "source": [
        "#location in drive where models are present.\n",
        "\n",
        "if(reuse == True):\n",
        "  gen.load_weights(\"gdrive/My Drive/newGAN/Generator.h5\")\n",
        "  dis.load_weights(\"gdrive/My Drive/newGAN/Discriminator.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ffMSHzQq16E"
      },
      "outputs": [],
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "  print(\"##############\")\n",
        "  print(\"For Epoch:\"+str(epoch))\n",
        "\n",
        "  g_loss = 0\n",
        "  d_loss = 0\n",
        "\n",
        "  print(\"Training Discriminator\")\n",
        "\n",
        "  i = shuffle(range(n_example))\n",
        "\n",
        "  dis.trainable = True\n",
        "  dis.compile(loss = \"binary_crossentropy\",optimizer = adams)\n",
        "\n",
        "  for j in range(dis_updates):\n",
        "\n",
        "    for b in range(batches):\n",
        "\n",
        "      x_batch = x[i[b*batch_size:(b+1)*batch_size]]\n",
        "      y_batch = y[i[b*batch_size:(b+1)*batch_size]]\n",
        "\n",
        "      pre_batch = gen.predict(x_batch)\n",
        "\n",
        "      d_loss += dis.train_on_batch([x_batch,y_batch],one)\n",
        "      d_loss += dis.train_on_batch([x_batch,pre_batch],zero)\n",
        "\n",
        "  print(\"Training Generator\")\n",
        "\n",
        "  dis.trainable = False\n",
        "  combined.compile(loss  = \"binary_crossentropy\", optimizer = adams)\n",
        "  dis.compile(loss = \"binary_crossentropy\",optimizer = adams)\n",
        "\n",
        "  for  j in range(gen_updates):\n",
        "\n",
        "    for b in range(batches):\n",
        "\n",
        "      x_batch = x[i[b*batch_size:(b+1)*batch_size]]\n",
        "      y_batch = y[i[b*batch_size:(b+1)*batch_size]]\n",
        "\n",
        "\n",
        "      #in case the mode collapse takes place....commenting next two lines might help.\n",
        "      #if b%4==3:\n",
        "        #gl,_,_ = combined.train_on_batch([x_batch,y_batch],[zero,one])\n",
        "\n",
        "      gl,_,_ = combined.train_on_batch([x_batch,y_batch],[one,one])\n",
        "      g_loss += gl\n",
        "\n",
        "  g_loss /= g_loss_factor\n",
        "  d_loss /= d_loss_factor\n",
        "\n",
        "  print(\"Discriminator Loss:\"+str(d_loss))\n",
        "  print(\"Generator loss:\"+str(g_loss))\n",
        "\n",
        "  genLoss.append(g_loss)\n",
        "  disLoss.append(d_loss)\n",
        "\n",
        "  gen.save_weights(\"gdrive/My Drive/newGAN/Generator.h5\")\n",
        "  dis.save_weights(\"gdrive/My Drive/newGAN/Discriminator.h5\")\n",
        "\n",
        "\n",
        "\n",
        "  plt_indices = np.random.randint(50000,size=3)\n",
        "  plt_a = x[plt_indices]\n",
        "  plt_b = gen.predict(plt_a)\n",
        "  plt_b = (plt_b+1)/2\n",
        "  plt_c = (y[plt_indices]+1)/2\n",
        "  fig = plot(plt_a,plt_b,plt_c,3)\n",
        "  plt.show()\n",
        "  plt.close(fig)\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(genLoss, c='r', label=\"Generator Loss\")\n",
        "plt.plot(disLoss, c='b', label=\"Discriminator Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "files.download('gdrive/My Drive/GANModels/Generator.h5')\n",
        "files.download('gdrive/My Drive/GANModels/Discriminator.h5')\n",
        "\n",
        "\n",
        "\n",
        "#for recieving mail on completion of training.\n",
        "server = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "server.starttls()\n",
        "server.login(\"************@gmail.com\", \"*********\")\n",
        "\n",
        "msg = \"COLAB WORK FINISH ALERT!\"\n",
        "server.sendmail(\"***********@gmail.com\", \"********@nirmauni.ac.in\", msg)\n",
        "server.quit()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GANForProject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}